{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 莊昱陽\n",
    "\n",
    "Student ID: 111061643\n",
    "\n",
    "GitHub ID: yuyangdanny\n",
    "\n",
    "Kaggle name: ABC\n",
    "\n",
    "Kaggle private scoreboard snapshot: (6-th place on private)\n",
    "\n",
    "[Snapshot](../pics/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the DM2023-Lab2-master. You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/t/09b1d0f3f8584d06848252277cb535f2) regarding Emotion Recognition on Twitter by this link https://www.kaggle.com/t/09b1d0f3f8584d06848252277cb535f2. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)   \n",
    "    Submit your last submission __BEFORE the deadline (Dec. 27th 11:59 pm, Wednesday)_. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 31th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third part\n",
    "See the framework architecture in report.pdf at \"/Homework/report.pdf\" for final result on private leader broad. <br>\n",
    "And I also provide extra experiment result in report.pdf. <br>\n",
    "The following code in this notebook is the function I tryed in all my experiments, and I'll also provide the example usage of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. build data reader pipeline class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    def __init__(self, sample_num=-1):\n",
    "        # sample_num = -1 for use whole training dataset\n",
    "        # sample_num > 0 for use sub sample dataset \n",
    "        self.sample_num = sample_num\n",
    "\n",
    "    def read_json(self, path):\n",
    "        with open(path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "            json_objects = []\n",
    "            for line in lines:\n",
    "                json_obj = json.loads(line)\n",
    "                json_objects.append(json_obj)\n",
    "        return pd.DataFrame(json_objects)\n",
    "\n",
    "    def read_csv(self, path):\n",
    "        return pd.read_csv(path)\n",
    "\n",
    "    def process(self, tweets_df, id_df, emo_df):\n",
    "        df = tweets_df\n",
    "        flattened_df = pd.json_normalize(df['_source'])\n",
    "        df = pd.concat([df, flattened_df], axis=1)\n",
    "        df.drop('_source', axis=1, inplace=True)\n",
    "        df.drop('_crawldate', axis=1, inplace=True)\n",
    "        df.drop('_index', axis=1, inplace=True)\n",
    "        df.drop('_type', axis=1, inplace=True)\n",
    "        df['tweet.hashtags'] = df['tweet.hashtags'].apply(lambda x: ' '.join(x))\n",
    "        df = df.rename(columns={'tweet.tweet_id': 'tweet_id', 'tweet.text': 'text', 'tweet.hashtags': 'hashtags'})\n",
    "        merged_df = pd.merge(df, id_df, on='tweet_id', how='inner')\n",
    "        train_df = merged_df[merged_df['identification'] == 'train']\n",
    "        train_df = pd.merge(train_df, emo_df, on='tweet_id', how='inner')\n",
    "        test_df = merged_df[merged_df['identification'] == 'test']\n",
    "        train_df.drop('identification', axis=1, inplace=True)\n",
    "        test_df.drop('identification', axis=1, inplace=True)\n",
    "        test_df.rename(columns={'tweet_id': 'id'})\n",
    "        \n",
    "        possible_labels = train_df['emotion'].unique()\n",
    "        label_dict = {}\n",
    "        for index, possible_label in tqdm(enumerate(possible_labels)):\n",
    "            label_dict[possible_label] = index\n",
    "        train_df['label'] = train_df.emotion.replace(label_dict)\n",
    "\n",
    "        # Use sample dataframe of not\n",
    "        if self.sample_num == -1:\n",
    "            return train_df, test_df, label_dict\n",
    "        else:\n",
    "            return train_df.head(self.sample_num), test_df, label_dict\n",
    "    \n",
    "    def train_val_split(self, df):\n",
    "        X_train, X_val, y_train, y_val = train_test_split(df.index.values, \n",
    "                                                  df.label.values, \n",
    "                                                  test_size=0.15,\n",
    "                                                  random_state=17, \n",
    "                                                  stratify=df.label.values)\n",
    "        df['data_type'] = ['not_set'] * df.shape[0]\n",
    "        df.loc[X_train, 'data_type'] = 'train'\n",
    "        df.loc[X_val, 'data_type'] = 'val'\n",
    "        \n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build Preprocessing pipeline class (for feature engineering)\n",
    "\n",
    "1. I tryed 2 different feature engineering method to deal with the 'text' and 'hashtag' column feature\n",
    "    1. (MyPreProcessor): Rule from scratch\n",
    "    2. (EkphrasisPreProcessor): Rule from ekphrasis (https://github.com/cbaziotis/ekphrasis)\n",
    "    \n",
    "So I design 2 different class to build the preprocessing pipeline call 'MyPreProcessor' and 'EkphrasisPreProcessor'\n",
    "And for the final leaderbroad result is by using the rule of 'MyPreProcessor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/Yuyang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/Yuyang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/Yuyang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Here are some example for using the preprocessor \\n1. Use MyPreProcessor:\\n    preprocessor = MyPreProcessor()\\n    tqdm.pandas()\\n    train_df['text'] = train_df['text'].progress_apply(preprocessor)\\n\\n2. Use EkphrasisPreProcessor\\n    preprocessor = EkphrasisPreProcessor()\\n    tqdm.pandas()\\n    train_df['text'] = train_df['text'].progress_apply(preprocessor.preprocess_tweet)\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import Tokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from ekphrasis.dicts.noslang.slangdict import slangdict\n",
    "\n",
    "class MyPreProcessor:\n",
    "    '''\n",
    "    Main process pipeline is in __call__ function, so you can jump into __call__ funciton to see the detail\n",
    "    '''\n",
    "    def __init__(self) -> None:\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def lemma_traincorpus(self, text):\n",
    "        words = word_tokenize(text)\n",
    "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    def contractions(self, text):\n",
    "        text = re.sub(r\"he's\", \"he is\", text)\n",
    "        text = re.sub(r'\\b(u)\\b', 'you', text)\n",
    "        text = re.sub(r\"there's\", \"there is\", text)\n",
    "        text = re.sub(r\"We're\", \"We are\", text)\n",
    "        text = re.sub(r\"That's\", \"That is\", text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"they're\", \"they are\", text)\n",
    "        text = re.sub(r\"Can't\", \"Cannot\", text)\n",
    "        text = re.sub(r\"wasn't\", \"was not\", text)\n",
    "        text = re.sub(r\"don\\x89Ûªt\", \"do not\", text)\n",
    "        text = re.sub(r\"aren't\", \"are not\", text)\n",
    "        text = re.sub(r\"isn't\", \"is not\", text)\n",
    "        text = re.sub(r\"What's\", \"What is\", text)\n",
    "        text = re.sub(r\"haven't\", \"have not\", text)\n",
    "        text = re.sub(r\"hasn't\", \"has not\", text)\n",
    "        text = re.sub(r\"There's\", \"There is\", text)\n",
    "        text = re.sub(r\"He's\", \"He is\", text)\n",
    "        text = re.sub(r\"It's\", \"It is\", text)\n",
    "        text = re.sub(r\"You're\", \"You are\", text)\n",
    "        text = re.sub(r\"I'M\", \"I am\", text)\n",
    "        text = re.sub(r\"shouldn't\", \"should not\", text)\n",
    "        text = re.sub(r\"wouldn't\", \"would not\", text)\n",
    "        text = re.sub(r\"i'm\", \"I am\", text)\n",
    "        text = re.sub(r\"I\\x89Ûªm\", \"I am\", text)\n",
    "        text = re.sub(r\"I'm\", \"I am\", text)\n",
    "        text = re.sub(r\"Isn't\", \"is not\", text)\n",
    "        text = re.sub(r\"Here's\", \"Here is\", text)\n",
    "        text = re.sub(r\"you've\", \"you have\", text)\n",
    "        text = re.sub(r\"you\\x89Ûªve\", \"you have\", text)\n",
    "        text = re.sub(r\"we're\", \"we are\", text)\n",
    "        text = re.sub(r\"what's\", \"what is\", text)\n",
    "        text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "        text = re.sub(r\"we've\", \"we have\", text)\n",
    "        text = re.sub(r\"it\\x89Ûªs\", \"it is\", text)\n",
    "        text = re.sub(r\"doesn\\x89Ûªt\", \"does not\", text)\n",
    "        text = re.sub(r\"It\\x89Ûªs\", \"It is\", text)\n",
    "        text = re.sub(r\"Here\\x89Ûªs\", \"Here is\", text)\n",
    "        text = re.sub(r\"who's\", \"who is\", text)\n",
    "        text = re.sub(r\"I\\x89Ûªve\", \"I have\", text)\n",
    "        text = re.sub(r\"y'all\", \"you all\", text)\n",
    "        text = re.sub(r\"can\\x89Ûªt\", \"cannot\", text)\n",
    "        text = re.sub(r\"would've\", \"would have\", text)\n",
    "        text = re.sub(r\"it'll\", \"it will\", text)\n",
    "        text = re.sub(r\"we'll\", \"we will\", text)\n",
    "        text = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", text)\n",
    "        text = re.sub(r\"We've\", \"We have\", text)\n",
    "        text = re.sub(r\"he'll\", \"he will\", text)\n",
    "        text = re.sub(r\"Y'all\", \"You all\", text)\n",
    "        text = re.sub(r\"Weren't\", \"Were not\", text)\n",
    "        text = re.sub(r\"Didn't\", \"Did not\", text)\n",
    "        text = re.sub(r\"they'll\", \"they will\", text)\n",
    "        text = re.sub(r\"they'd\", \"they would\", text)\n",
    "        text = re.sub(r\"DON'T\", \"DO NOT\", text)\n",
    "        text = re.sub(r\"That\\x89Ûªs\", \"That is\", text)\n",
    "        text = re.sub(r\"they've\", \"they have\", text)\n",
    "        text = re.sub(r\"i'd\", \"I would\", text)\n",
    "        text = re.sub(r\"should've\", \"should have\", text)\n",
    "        text = re.sub(r\"You\\x89Ûªre\", \"You are\", text)\n",
    "        text = re.sub(r\"where's\", \"where is\", text)\n",
    "        text = re.sub(r\"Don\\x89Ûªt\", \"Do not\", text)\n",
    "        text = re.sub(r\"we'd\", \"we would\", text)\n",
    "        text = re.sub(r\"i'll\", \"I will\", text)\n",
    "        text = re.sub(r\"weren't\", \"were not\", text)\n",
    "        text = re.sub(r\"They're\", \"They are\", text)\n",
    "        text = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", text)\n",
    "        text = re.sub(r\"you\\x89Ûªll\", \"you will\", text)\n",
    "        text = re.sub(r\"I\\x89Ûªd\", \"I would\", text)\n",
    "        text = re.sub(r\"let's\", \"let us\", text)\n",
    "        text = re.sub(r\"it's\", \"it is\", text)\n",
    "        text = re.sub(r\"can't\", \"cannot\", text)\n",
    "        text = re.sub(r\"don't\", \"do not\", text)\n",
    "        text = re.sub(r\"you're\", \"you are\", text)\n",
    "        text = re.sub(r\"i've\", \"I have\", text)\n",
    "        text = re.sub(r\"that's\", \"that is\", text)\n",
    "        text = re.sub(r\"i'll\", \"I will\", text)\n",
    "        text = re.sub(r\"doesn't\", \"does not\", text)\n",
    "        text = re.sub(r\"i'd\", \"I would\", text)\n",
    "        text = re.sub(r\"didn't\", \"did not\", text)\n",
    "        text = re.sub(r\"ain't\", \"am not\", text)\n",
    "        text = re.sub(r\"you'll\", \"you will\", text)\n",
    "        text = re.sub(r\"I've\", \"I have\", text)\n",
    "        text = re.sub(r\"Don't\", \"do not\", text)\n",
    "        text = re.sub(r\"I'll\", \"I will\", text)\n",
    "        text = re.sub(r\"I'd\", \"I would\", text)\n",
    "        text = re.sub(r\"Let's\", \"Let us\", text)\n",
    "        text = re.sub(r\"you'd\", \"You would\", text)\n",
    "        text = re.sub(r\"It's\", \"It is\", text)\n",
    "        text = re.sub(r\"Ain't\", \"am not\", text)\n",
    "        text = re.sub(r\"Haven't\", \"Have not\", text)\n",
    "        text = re.sub(r\"Could've\", \"Could have\", text)\n",
    "        text = re.sub(r\"youve\", \"you have\", text)  \n",
    "        text = re.sub(r\"donå«t\", \"do not\", text) \n",
    "\n",
    "        abbreviations = {\n",
    "            \"aren't\" : \"are not\",\n",
    "            \"can't\" : \"cannot\",\n",
    "            \"couldn't\" : \"could not\",\n",
    "            \"didn't\" : \"did not\",\n",
    "            \"doesn't\" : \"does not\",\n",
    "            \"don't\" : \"do not\",\n",
    "            \"hadn't\" : \"had not\",\n",
    "            \"hasn't\" : \"has not\",\n",
    "            \"haven't\" : \"have not\",\n",
    "            \"he'd\" : \"he had\",\n",
    "            \"he'll\" : \"he will\",\n",
    "            \"he's\" : \"he is\",\n",
    "            \"I'd\" : \"I had\",\n",
    "            \"I'll\" : \"I will\",\n",
    "            \"I'm\": \"I am\",\n",
    "            \"I've\" : \"I have\",\n",
    "            \"isn't\" : \"is not\",\n",
    "            \"let's\" : \"let us\",\n",
    "            \"mightn't\" : \"might not\",\n",
    "            \"mustn't\" : \"must not\",\n",
    "            \"shan't\" : \"shall not\",\n",
    "            \"she'd\" : \"she had\",\n",
    "            \"she'll\" : \"she will\",\n",
    "            \"she's\" : \"she is\",\n",
    "            \"shouldn't\" : \"should not\",\n",
    "            \"that's\" : \"that is\",\n",
    "            \"there's\" : \"there is\",\n",
    "            \"they'd\" : \"they had\",\n",
    "            \"they'll\" : \"they will\",\n",
    "            \"they're\" : \"they are\",\n",
    "            \"they've\" : \"they have\",\n",
    "            \"we'd\" : \"we had\",\n",
    "            \"we're\" : \"we are\",\n",
    "            \"we've\" : \"we have\",\n",
    "            \"weren't\" : \"were not\",\n",
    "            \"what'll\" : \"what will\",\n",
    "            \"what're\" : \"what are\",\n",
    "            \"what's\" : \"what is\",\n",
    "            \"what've\" : \"what have\",\n",
    "            \"where's\" : \"where is\",\n",
    "            \"who's\" : \"who had\",\n",
    "            \"who'll\" : \"who will\",\n",
    "            \"who're\" : \"who are\",\n",
    "            \"who's\" : \"who is\",\n",
    "            \"who've\" : \"who have\",\n",
    "            \"won't\" : \"will not\",\n",
    "            \"wouldn't\" : \"would not\",\n",
    "            \"wouldnt\" : \"would not\",\n",
    "            \"you'd\" : \"you had\",\n",
    "            \"you'll\" : \"you will\",\n",
    "            \"you're\" : \"you are\",\n",
    "            \"you've\" : \"you have\",\n",
    "            \"arent\" : \"are not\",\n",
    "            \"cant\" : \"cannot\",\n",
    "            \"couldnt\" : \"could not\",\n",
    "            \"didnt\" : \"did not\",\n",
    "            \"doesnt\" : \"does not\",\n",
    "            \"dont\" : \"do not\",\n",
    "            \"hadnt\" : \"had not\",\n",
    "            \"hasnt\" : \"has not\",\n",
    "            \"havent\" : \"have not\",\n",
    "            \"Id\" : \"I had\",\n",
    "            \"Ill\" : \"I will\",\n",
    "            \"Im\": \"I am\",\n",
    "            \"Ive\" : \"I have\",\n",
    "            \"isnt\" : \"is not\",\n",
    "            \"lets\" : \"let us\",\n",
    "            \"mightnt\" : \"might not\",\n",
    "            \"mustnt\" : \"must not\",\n",
    "            \"shouldnt\" : \"should not\",\n",
    "            \"werent\" : \"were not\",\n",
    "            \"gonna\" : \"going to\",\n",
    "            \"imma\" : \"i am going to\"\n",
    "        }\n",
    "\n",
    "        for key, value in abbreviations.items():\n",
    "            text = re.sub(re.escape(key), value, text)\n",
    "        return text\n",
    "    \n",
    "    def rm_space(self, text):\n",
    "        text = text.strip()\n",
    "        text = text.split()\n",
    "        return ' '.join(text)\n",
    "\n",
    "    def clean(self, text):\n",
    "        text = re.sub(r\"&gt;\", \">\", text)\n",
    "        text = re.sub(r\"&lt;\", \"<\", text)\n",
    "        text = re.sub(r\"&amp;\", \"&\", text)\n",
    "        text = re.sub(r'&[^ ]*', '', text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def __call__(self, text):\n",
    "        # rm html\n",
    "        html = re.compile(r'<.*?>')\n",
    "        text = html.sub(r'',text)\n",
    "\n",
    "        # rm URL\n",
    "        url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        text = url.sub(r'',text)\n",
    "\n",
    "        # rm informal space\n",
    "        text = self.rm_space(text)\n",
    "\n",
    "        # clean\n",
    "        text = self.clean(text)\n",
    "\n",
    "        # rm metion\n",
    "        text = re.sub(r'@\\S+', '', text)\n",
    "\n",
    "        # Split connective\n",
    "        text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "\n",
    "        # lower case transformation\n",
    "        text = text.lower()\n",
    "\n",
    "        # contractions\n",
    "        text = self.contractions(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "class EkphrasisPreProcessor:\n",
    "    \"\"\"\n",
    "    This class does some cleaning and normalization prior to BPE tokenization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.text_processor = TextPreProcessor(\n",
    "            # terms that will be normalized\n",
    "            normalize=[\n",
    "                \"url\",\n",
    "                \"email\",\n",
    "                \"phone\",\n",
    "                \"user\",\n",
    "                \"time\",\n",
    "                \"date\",\n",
    "                'percent',\n",
    "                'money'\n",
    "            ],\n",
    "            # terms that will be annotated\n",
    "            # annotate={\"repeated\", \"elongated\"},\n",
    "            annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "                    'emphasis'},\n",
    "            # corpus from which the word statistics are going to be used\n",
    "            # for word segmentation\n",
    "            segmenter=\"twitter\",\n",
    "            # corpus from which the word statistics are going to be used\n",
    "            # for spell correction\n",
    "            spell_correction=True,\n",
    "            corrector=\"twitter\",\n",
    "            unpack_hashtags=False,  # perform word segmentation on hashtags\n",
    "            unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "            spell_correct_elong=True,  # spell correction for elongated words\n",
    "            fix_bad_unicode=True,\n",
    "            tokenizer=Tokenizer(lowercase=True).tokenize,\n",
    "            # list of dictionaries, for replacing tokens extracted from the text,\n",
    "            # with other expressions. You can pass more than one dictionaries.\n",
    "            dicts=[emoticons, slangdict],\n",
    "        )\n",
    "\n",
    "    def preprocess_tweet(self, tweet):\n",
    "        return \" \".join(self.text_processor.pre_process_doc(tweet))\n",
    "    \n",
    "    # this will return the tokenized text     \n",
    "    def __call__(self, tweet):\n",
    "        return self.text_processor.pre_process_doc(tweet)\n",
    "    \n",
    "\n",
    "''' Here are some example for using the preprocessor \n",
    "1. Use MyPreProcessor:\n",
    "    preprocessor = MyPreProcessor()\n",
    "    tqdm.pandas()\n",
    "    train_df['text'] = train_df['text'].progress_apply(preprocessor)\n",
    "\n",
    "2. Use EkphrasisPreProcessor\n",
    "    preprocessor = EkphrasisPreProcessor()\n",
    "    tqdm.pandas()\n",
    "    train_df['text'] = train_df['text'].progress_apply(preprocessor.preprocess_tweet)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build model training pipeline class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def prepare_train_val_dataloader(self, df, tokenizer, batch_size=64):\n",
    "        encoded_data_train = tokenizer.batch_encode_plus(\n",
    "            tqdm(df[df.data_type=='train'].text.values), \n",
    "            add_special_tokens=True, \n",
    "            return_attention_mask=True, \n",
    "            pad_to_max_length=True, \n",
    "            max_length=256, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        encoded_data_val = tokenizer.batch_encode_plus(\n",
    "            tqdm(df[df.data_type=='val'].text.values), \n",
    "            add_special_tokens=True, \n",
    "            return_attention_mask=True, \n",
    "            pad_to_max_length=True, \n",
    "            max_length=256, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids_train = encoded_data_train['input_ids']\n",
    "        attention_masks_train = encoded_data_train['attention_mask']\n",
    "        labels_train = torch.tensor(df[df.data_type=='train'].label.values)\n",
    "        input_ids_val = encoded_data_val['input_ids']\n",
    "        attention_masks_val = encoded_data_val['attention_mask']\n",
    "        labels_val = torch.tensor(df[df.data_type=='val'].label.values)\n",
    "\n",
    "        dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "        dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "        dataloader_train = DataLoader(dataset_train, \n",
    "                        sampler=RandomSampler(dataset_train), \n",
    "                        batch_size=batch_size)\n",
    "        dataloader_validation = DataLoader(dataset_val, \n",
    "                        sampler=SequentialSampler(dataset_val), \n",
    "                        batch_size=batch_size)\n",
    "\n",
    "        return dataloader_train, dataloader_validation\n",
    "\n",
    "    def build_model(self, model_name, label_dict):\n",
    "        if model_name == 'bert-base-uncased' or 'prajjwal1/bert-tiny':\n",
    "            model = BertForSequenceClassification.from_pretrained(model_name,\n",
    "                                        num_labels=len(label_dict),\n",
    "                                        output_attentions=False,\n",
    "                                        output_hidden_states=False)\n",
    "        elif model_name == 'roberta-base' or 'distilroberta-base':\n",
    "            model = RobertaForSequenceClassification.from_pretrained(model_name,\n",
    "                                    num_labels=len(label_dict),\n",
    "                                    output_attentions=False,\n",
    "                                    output_hidden_states=False)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def _setSeed(self, seed_id=42):\n",
    "        random.seed(seed_id)\n",
    "        np.random.seed(seed_id)\n",
    "        torch.manual_seed(seed_id)\n",
    "        torch.cuda.manual_seed_all(seed_id)\n",
    "\n",
    "    def train(self, model, model_name, dataloader_train, dataloader_validation, epochs=2):\n",
    "        self._setSeed()\n",
    "\n",
    "        optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train)*epochs)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "\n",
    "        for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "            model.train()\n",
    "            loss_train_total = 0\n",
    "            progress_bar = tqdm(dataloader_train, desc= f'Model: {model_name}' + 'Epoch {:1d}/{:1d}'.format(epoch, epochs), leave=False, disable=False)\n",
    "            for batch in progress_bar:\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch = tuple(b.to(device) for b in batch)\n",
    "                inputs = {'input_ids':      batch[0],\n",
    "                        'attention_mask': batch[1],\n",
    "                        'labels':         batch[2],\n",
    "                        }       \n",
    "\n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "                loss = outputs[0]\n",
    "                loss_train_total += loss.item()\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "                \n",
    "                \n",
    "            torch.save(model.state_dict(), f'finetuned_epoch_{epoch}.model')\n",
    "            tqdm.write(f'\\nEpoch {epoch}')\n",
    "            loss_train_avg = loss_train_total/len(dataloader_train)             \n",
    "            tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "            val_loss, predictions, true_vals = self.evaluate(model, dataloader_validation, device)\n",
    "            val_f1 = self.f1_score_func(predictions, true_vals)\n",
    "            tqdm.write(f'Validation loss: {val_loss}')\n",
    "            tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
    "\n",
    "        torch.save(model, f'whole_finetuned_model.pth')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def evaluate(self, model, dataloader_val, device):\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        loss_val_total = 0\n",
    "        predictions, true_vals = [], []\n",
    "        \n",
    "        for batch in dataloader_val:\n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                    'attention_mask': batch[1],\n",
    "                    'labels':         batch[2],\n",
    "                    }\n",
    "\n",
    "            with torch.no_grad():        \n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            loss_val_total += loss.item()\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = inputs['labels'].cpu().numpy()\n",
    "            predictions.append(logits)\n",
    "            true_vals.append(label_ids)\n",
    "        \n",
    "        loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        true_vals = np.concatenate(true_vals, axis=0)\n",
    "                \n",
    "        return loss_val_avg, predictions, true_vals\n",
    "\n",
    "    def f1_score_func(self, preds, labels):\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return f1_score(labels_flat, preds_flat, average='weighted')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate submition data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_submit_csv(test_df, preprocessor, tokenizer, model, model_name, device):\n",
    "\n",
    "    tqdm_notebook.pandas()\n",
    "    test_df['text'] = test_df['text'].progress_apply(preprocessor)\n",
    "    test_df = test_df.drop('_score', axis=1).drop('hashtags', axis=1)\n",
    "    test = test_df.set_index('tweet_id').T.to_dict('list')\n",
    "    label = []\n",
    "\n",
    "    print(\"Start to parse to leader broad ! \")\n",
    "    for id in tqdm(test):\n",
    "        sentence = test[id]\n",
    "\n",
    "        inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "\n",
    "        # to gpu\n",
    "        ids = inputs[\"input_ids\"].to(device)\n",
    "        mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "        # to model\n",
    "        outputs = model(ids, mask)\n",
    "        logits = outputs[0]\n",
    "\n",
    "        active_logits = logits.view(-1, model.num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1)\n",
    "\n",
    "        tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "        ids_to_labels = {'0':'anticipation', '1':'sadness', '2':'fear', '3':'joy', '4':'anger', '5':'trust', '6':'disgust', '7':'surprise'}\n",
    "        token_predictions = ids_to_labels[str(flattened_predictions.cpu().numpy()[0])]\n",
    "        label.append(token_predictions)\n",
    "\n",
    "    submission = test_df.drop('_score', axis=1).drop('hashtags', axis=1).drop('text', axis=1)\n",
    "    submission = submission.rename({'tweet_id': 'id'})\n",
    "    submission['emotion'] = 0\n",
    "    submission = submission.assign(emotion = label)\n",
    "    submission = submission.drop(['text'], axis=1)\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(f'Save model at submission.csv from model: {model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole pipeline execution example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Yuyang/anaconda3/envs/dataminingLab2/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "8it [00:00, 174762.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set file path\n",
    "json_file = \"./kaggle/input/tweets_DM.json\"\n",
    "data_identification_file = \"./kaggle/input/data_identification.csv\"\n",
    "emotion_file = \"./kaggle/input/emotion.csv\"\n",
    "\n",
    "# Read 3 raw data into dataframe\n",
    "datareader = DataReader()\n",
    "id_df = datareader.read_csv(data_identification_file)\n",
    "id_df_train = id_df[id_df['identification'] == 'train']\n",
    "id_df_test = id_df[id_df['identification'] == 'test']\n",
    "emo_df = datareader.read_csv(emotion_file)\n",
    "\n",
    "# Prepare train val test dataset\n",
    "train_df, test_df, label_dict = datareader.process(datareader.read_json(json_file), id_df, emo_df)\n",
    "df = datareader.train_val_split(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocess data (feature engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_notebook.pandas()\n",
    "\n",
    "# Choose Feature engineer method\n",
    "FE_method = '1' # '0' for feature engineer from scratch '1' for ekphrasis\n",
    "\n",
    "# Process feature engineer method\n",
    "if FE_method == '0':\n",
    "    preprocessor = MyPreProcessor()\n",
    "elif FE_method == '1':\n",
    "    preprocessor = EkphrasisPreProcessor()\n",
    "    preprocessor = preprocessor.preprocess_tweet\n",
    "\n",
    "train_df['text'] = train_df['text'].progress_apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Choose model and tokenizer\n",
    "We can choose 4 different model types here, include:\n",
    "1. bert-tiny\n",
    "2. bert-base-uncased\n",
    "3. roberta-base\n",
    "4. distilroberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"prajjwal1/bert-tiny\" # \"roberta-base\", \"distilroberta-base\", \"prajjwal1/bert-tiny\", \"bert-base-uncased\"\n",
    "\n",
    "if model_name == 'prajjwal1/bert-tiny':\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "    epochs = 1\n",
    "elif model_name == 'bert-base-uncased':\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "    epochs = 3\n",
    "elif model_name == 'roberta-base' or 'distilroberta-base':\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "    epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1237228 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/Yuyang/anaconda3/envs/dataminingLab2/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2383: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "100%|██████████| 1237228/1237228 [14:08<00:00, 1457.96it/s]\n",
      "100%|██████████| 218335/218335 [02:29<00:00, 1461.92it/s]\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/Yuyang/anaconda3/envs/dataminingLab2/lib/python3.7/site-packages/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "  0%|          | 0/1 [07:34<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 1.6145336296699961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [08:04<00:00, 484.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 1.5123645513026962\n",
      "F1 Score (Weighted): 0.38490764461491617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer()\n",
    "dataloader_train, dataloader_validation = trainer.prepare_train_val_dataloader(df, tokenizer, batch_size=256)\n",
    "model = trainer.build_model(model_name, label_dict)\n",
    "model = trainer.train(model, model_name, dataloader_train, dataloader_validation, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gen_submit_csv(test_df, preprocessor, tokenizer, model, model_name, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ensumble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "\n",
    "# Write file path here\n",
    "file1 = \"/mnt/bel/Code/NLP/DM2023-Lab2/new_tryed___bert-base-uncased_submission.csv\"\n",
    "file2 = \"/mnt/bel/Code/NLP/DM2023-Lab2/new_tryed___distilroberta-base_submission.csv\"\n",
    "file3 = \"/mnt/bel/Code/NLP/DM2023-Lab2/new_tryed___roberta-base_submission.csv\"\n",
    "file4 = \"/mnt/bel/Code/NLP/DM2023-Lab2/new_tryed_ekphrasis_distilroberta-base_submission.csv\"\n",
    "file5 = \"/mnt/bel/Code/NLP/DM2023-Lab2/new_tryed_ekphrasis_roberta-base_submission.csv\"\n",
    "\n",
    "# Convert into dataframe and rename emotion colunm name\n",
    "df1 = pd.read_csv(file1)\n",
    "df2 = pd.read_csv(file2)\n",
    "df3 = pd.read_csv(file3)\n",
    "df4 = pd.read_csv(file4)\n",
    "df5 = pd.read_csv(file5)\n",
    "df1 = df1.rename(columns={'emotion': 'emotion_df1'})\n",
    "df2 = df2.rename(columns={'emotion': 'emotion_df2'})\n",
    "df3 = df3.rename(columns={'emotion': 'emotion_df3'})\n",
    "df4 = df4.rename(columns={'emotion': 'emotion_df4'})\n",
    "df5 = df5.rename(columns={'emotion': 'emotion_df5'})\n",
    "\n",
    "# Merge all dataframe\n",
    "merged_df = pd.merge(df1[['id', 'emotion_df1']], df2[['id', 'emotion_df2']], on='id', how='outer')\n",
    "merged_df = pd.merge(merged_df, df3[['id', 'emotion_df3']], on='id', how='outer')\n",
    "merged_df = pd.merge(merged_df, df4[['id', 'emotion_df4']], on='id', how='outer')\n",
    "merged_df = pd.merge(merged_df, df5[['id', 'emotion_df5']], on='id', how='outer')\n",
    "\n",
    "\n",
    "def majority_vote(row):\n",
    "    values = row[['emotion_df1', 'emotion_df2', 'emotion_df3', 'emotion_df4', 'emotion_df5']]\n",
    "    \n",
    "    mode_values = values.mode()\n",
    "\n",
    "    # use majority_vote result if mode less than amount of df\n",
    "    if len(mode_values) < 5:\n",
    "        mode_value = mode_values.iloc[0]\n",
    "    # if we didn't get real majority here, we just take first df result\n",
    "    else:\n",
    "        mode_value = row['emotion_df1']\n",
    "    \n",
    "    return mode_value\n",
    "\n",
    "# Utils majority vote method and output csv file\n",
    "tqdm_notebook.pandas()\n",
    "merged_df['emotion'] = merged_df.progress_apply(majority_vote, axis=1)\n",
    "result_df = merged_df[['id', 'emotion']]\n",
    "print(\"Size: \", result_df.shape)\n",
    "result_df.to_csv(\"ensumble_submission.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
